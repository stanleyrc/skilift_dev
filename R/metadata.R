#' @name initialize_metadata_columns
#' @title Initialize Metadata Columns
#' @description Creates a data.table with all expected columns initialized to NA
#' @param pair The sample pair identifier
#' @return A data.table with all possible columns initialized to NA
initialize_metadata_columns <- function(pair) {
    if (is.null(pair)) {
        stop("pair must be a character string")
    }
    if (!is.character(pair) && is.integer(pair)) {
        warning("pair is an integer, converting to character")
        pair <- as.character(pair)
    }
    if (!is.character(pair)) {
        stop("pair must be a character string")
    }
    if (nchar(pair) == 0) {
        stop("pair cannot be empty")
    }
    
    dt <- data.table(
        # Basic metadata
        pair = pair,
        tumor_type = NA_character_,
        tumor_details = NA_character_,
        disease = NA_character_,
        primary_site = NA_character_,
        inferred_sex = NA_character_,
        
        # Coverage QC
        coverage_qc = list(NULL),
        
        # Variant counts
        snv_count = NA_integer_,
        snv_count_normal_vaf_greater0 = NA_integer_,
        
        # Coverage parameters
        cov_slope = NA_real_,
        cov_intercept = NA_real_,
        
        # Signature lists
        deconstructsigs_sbs_fraction = list(NULL),
        sigprofiler_indel_fraction = list(NULL),
        sigprofiler_sbs_count = list(NULL),
        sigprofiler_sbs_fraction = list(NULL),
        
        # HRD scores
        hrd_score = NA_real_,
        hrd = list(NULL),
        b1_2 = NA_real_,
        b1 = NA_real_,
        b2 = NA_real_,
        tmb = NA_real_,  # Add this line

        conpair_contamination = NA_real_,
        conpair_concordance = NA_real_
    )
    return(dt)
}

#' @name add_basic_metadata
#' @title Add Basic Metadata
#' @description
#' Adds basic metadata information such as tumor type, tumor details, disease, and primary site.
#'
#' @param metadata A data.table containing metadata.
#' @param tumor_type The type of tumor.
#' @param tumor_details Details about the tumor.
#' @param disease The disease associated with the sample.
#' @param primary_site The primary site of the tumor.
#' @return Updated metadata with basic information added.
add_basic_metadata <- function(
    metadata,
    input_tumor_type,
    input_tumor_details,
    input_disease,
    input_primary_site
) {
    # Input validation
    if (is.null(metadata)) {
        stop("metadata cannot be NULL")
    }
    if (!is.data.table(metadata)) {
        stop("metadata must be a data.table")
    }
    
    # Validate tumor_type if provided
    if (!is.null(input_tumor_type)) {
        if (!is.character(input_tumor_type)) {
            stop("tumor_type must be NULL or a character string")
        }
        metadata[, tumor_type := input_tumor_type]
    }

    # Validate tumor_details if provided
    if (!is.null(input_tumor_details)) {
        if (!is.character(input_tumor_details)) {
            stop("tumor_details must be NULL or a character string")
        }
        metadata[, tumor_details := input_tumor_details]
    }

    # Validate disease if provided
    if (!is.null(input_disease)) {
        if (!is.character(input_disease)) {
            stop("disease must be NULL or a character string")
        }
        metadata[, disease := input_disease]
    }
    
    # Validate primary_site if provided
    if (!is.null(input_primary_site)) {
        if (!is.character(input_primary_site)) {
            stop("primary_site must be NULL or a character string")
        }
        metadata[, primary_site := input_primary_site]
    }
    
    return(metadata)
}

#' @name add_sex_information
#' @title Add Sex Information
#' @description
#' Adds inferred sex information to the metadata based on various inputs.
#'
#' @param metadata A data.table containing metadata.
#' @param input_inferred_sex The inferred sex of the sample.
#' @param jabba_gg A file path to the jabba_gg data.
#' @param tumor_coverage Coverage data for the tumor.
#' @return Updated metadata with inferred sex information added.
add_sex_information <- function(
    metadata,
    input_inferred_sex = NULL,
    jabba_gg = NULL,
    tumor_coverage = NULL 
) {
    # Direct sex specification
    if (!is.null(input_inferred_sex)) {
        if (!input_inferred_sex %in% c("Male", "Female")) {
            warning("inferred sex must be one of `Male` or `Female`")
        }
        metadata[, inferred_sex := input_inferred_sex]
        return(metadata)
    }
    
    # Infer from jabba_gg if available
    if (!is.null(jabba_gg)) {
        gg <- process_jabba(jabba_gg)
        ncn.x <- gg$nodes$dt[
            (seqnames == "X" | seqnames == "chrX"),
            weighted.mean(cn, w = end - start + 1, na.rm = TRUE)
        ]
        metadata[, inferred_sex := ifelse(ncn.x < 1.4, "Male", "Female")]
        return(metadata)
    }
    
    # Infer from tumor coverage if available
    if (!is.null(tumor_coverage)) {
        t_cov <- gGnome:::readCov(tumor_coverage)
        ncn.x <- unique(gr2dt(t_cov)[, foreground.chr := mean(foreground), by = seqnames][, .(seqnames, foreground.chr)])[seqnames %in% c("X", "chrX")]$foreground.chr
        if (length(ncn.x) == 0) {
            warning("Could not extract X chromosome coverage from tumor coverage data")
            return(metadata)
        }
        metadata[, inferred_sex := ifelse(ncn.x < 0.7, "Male", "Female")]
        return(metadata)
    }
    
    return(metadata)
}

#' @name dlrs
#' @title derivative log ratio spread
#' @description
#'
#' function to get the dlrs for coverage data
#' used in meta_data_json
#'
#' @param x foreground from dryclean
#' @return dlrs
#' @export
#' @author Joel Rosiene
dlrs <- function(x) {
    nx <- length(x)
    if (nx < 3) {
        stop("Vector length>2 needed for computation")
    }
    tmp <- embed(x, 2)
    diffs <- tmp[, 2] - tmp[, 1]
    dlrs <- IQR(diffs, na.rm = TRUE) / (sqrt(2) * 1.34)
    return(dlrs)
}

#' @name extract_metrics
#' @title Extract Metrics from QC File
#' @description Helper function to extract specified metrics from a QC file
#' @param qc_data Data.table containing QC metrics
#' @param metrics Named vector of column names to extract
#' @param pair Sample pair identifier
#' @return Data.table with extracted metrics
extract_metrics <- function(qc_data, metrics, pair) {
    # Verify all metrics exist in data
    missing_cols <- setdiff(metrics, names(qc_data))
    if (length(missing_cols) > 0) {
        message(sprintf("Missing columns in QC data: %s", paste(missing_cols, collapse = ", ")), "\nSetting to NA")
    }

    # Select and rename columns
    # result <- qc_data[, metrics, with = FALSE]
    # setnames(result, old = metrics, new = names(metrics))

    qc_data = as.list(qc_data)
    result = data.table(pair = pair)

    ix = seq_along(metrics)
    for (i in ix) {
        metric = metrics[i]
        is_metric_present = exists(metric, qc_data)
        val = NA_real_
        if (!is_metric_present) {
            ## result[[names(metrics)[metrics == metric]]] = NA_real_
            result = cbind(result, "rename___col___" = val)
            ## next
        }
        val = qc_data[[metric]]
        result = cbind(result, "rename___col___" = val)
        names(result)[names(result) == "rename___col___"] = names(metric)
        ## result[[names(metrics)[metrics == metric]]] = qc_data[[metric]]
    }
        
    # Add pair identifier
    # result[, pair := pair]
    
    return(result)
}

# #' @name process_qc_metrics
# #' @title Process Quality Control Metrics
# #' @description Processes quality control metrics from multiple QC files and combines them
# #' @param estimate_library_complexity Path to the estimate_library_complexity_metrics file
# #' @param alignment_summary_metrics Path to the alignment_summary_metrics file
# #' @param insert_size_metrics Path to the insert_size_metrics file
# #' @param wgs_metrics Path to the wgs_metrics file
# #' @param pair Sample pair identifier
# #' @return A list containing processed QC metrics
# process_qc_metrics <- function(
#     estimate_library_complexity,
#     alignment_summary_metrics,
#     insert_size_metrics,
#     tumor_wgs_metrics,
#     normal_wgs_metrics,
#     pair
# ) {

#     # Define metric mappings for each file type
#     complexity_metrics_cols <- c(
#         read_pairs_examined = "READ_PAIRS_EXAMINED",
#         read_pair_duplicates = "READ_PAIR_DUPLICATES",
#         read_pair_optical_duplicates = "READ_PAIR_OPTICAL_DUPLICATES",
#         percent_duplication = "PERCENT_DUPLICATION"
#     )
    
#     alignment_metrics_cols <- c(
#         total_reads = "TOTAL_READS",
#         pf_reads_aligned = "PF_READS_ALIGNED",
#         pf_aligned_bases = "PF_ALIGNED_BASES",
#         mean_read_length = "MEAN_READ_LENGTH"
#     )
    
#     insert_metrics_cols <- c(
#         # median_insert_size = "MEDIAN_INSERT_SIZE"
#         insert_size = "MEDIAN_INSERT_SIZE"
#     )
    
#     tumor_wgs_metrics_cols <- c(
#         # median_coverage = "MEAN_COVERAGE"
#         tumor_median_coverage = "MEDIAN_COVERAGE",
#         # pct_30x = "PCT_30X",
#         greater_than_or_equal_to_30x = "PCT_30X",
#         # pct_50x = "PCT_50X"
#         greater_than_or_equal_to_50x = "PCT_50X",
#         fraction_excluded = "PCT_EXC_TOTAL"
#     )

#     normal_wgs_metrics_cols <- c(
#         normal_median_coverage = "MEDIAN_COVERAGE",
#         fraction_excluded = "PCT_EXC_TOTAL"
#     )
    
#     test_file_is_present = function(x) {
#         (
#             !is.null(x)
#             && is.character(x)
#             && NROW(x) == 1
#             && file.exists(x)
#         )
#     }

#     # Read and extract metrics from each file
#     complexity_data = data.table(pair = character(0))
#     if (test_file_is_present(estimate_library_complexity)) {
#         complexity_data <- extract_metrics(
#             fread(estimate_library_complexity),
#             complexity_metrics_cols,
#             pair
#         )
#     }
    
#     alignment_data = data.table(pair = character(0))
#     if (test_file_is_present(alignment_summary_metrics)) {
#         alignment_data <- extract_metrics(
#             fread(alignment_summary_metrics)[CATEGORY=="PAIR"],
#             alignment_metrics_cols,
#             pair
#         )
#     }
    
#     insert_data = data.table(pair = character(0))
#     if (test_file_is_present(insert_size_metrics)) {
#         insert_data <- extract_metrics(
#             fread(insert_size_metrics)[PAIR_ORIENTATION == "FR"],
#             insert_metrics_cols,
#             pair
#         )
#     }

#     tumor_wgs_data = data.table(pair = character(0))
#     if (test_file_is_present(tumor_wgs_metrics)) {
#         tumor_wgs_data <- extract_metrics(
#             fread(tumor_wgs_metrics),
#             tumor_wgs_metrics_cols,
#             pair
#         )
#         tumor_wgs_data$tumor_median_coverage = round(
#             tumor_wgs_data$tumor_median_coverage / (1 - tumor_wgs_data$fraction_excluded)
#         )
#         tumor_wgs_data$fraction_excluded = NULL
#     }

#     normal_wgs_data = data.table(pair = character(0))
#     if (test_file_is_present(normal_wgs_metrics)) {
#         normal_wgs_data <- extract_metrics(
#             fread(normal_wgs_metrics),
#             normal_wgs_metrics_cols,
#             pair
#         )
#         normal_wgs_data$normal_median_coverage = round(
#             normal_wgs_data$normal_median_coverage / (1 - normal_wgs_data$fraction_excluded)
#         )
#         normal_wgs_data$fraction_excluded = NULL
#     }
    
#     # c("pair", "total_reads", "pf_reads_aligned", "pf_aligned_bases", 
#     # "mean_read_length", "median_insert_size", "median_coverage", 
#     # "pct_30x", "pct_50x", "percent_optical_duplication", 
#     # "percent_aligned", "percent_optical_dups_of_dups")
#     # Merge all metrics on pair
#     lst_to_merge = list(
#         complexity_data, 
#         alignment_data, 
#         insert_data, 
#         tumor_wgs_data,
#         normal_wgs_data
#     )
#     qc_metrics <- Reduce(function(x, y) {
#         data.table::merge.data.table(
#             x, y, by = "pair", 
#             all.x = TRUE, 
#             all.y = TRUE,
#             suffixes = c("_x", "_y")
#         )
#     }   , lst_to_merge)
    
#     # Calculate derivative metrics
#     # Separating out and writing long way for robustness to
#     # missing data.
#     qc_metrics$m_reads = qc_metrics$total_reads / 1e6
#     qc_metrics$m_reads_mapped = qc_metrics$pf_reads_aligned / 1e6
#     qc_metrics$percent_optical_duplication = (
#         qc_metrics$read_pair_optical_duplicates /
#         qc_metrics$read_pairs_examined
#     )
#     qc_metrics$percent_aligned = (
#         qc_metrics$pf_aligned_bases / 
#         ( qc_metrics$total_reads * qc_metrics$mean_read_length )
#     )
#     qc_metrics$percent_optical_dups_of_dups = (
#         qc_metrics$read_pair_optical_duplicates / 
#         qc_metrics$read_pair_duplicates
#     )
#     # # FIXME, need to account for tumor and normal
#     # qc_metrics$normal_median_coverage = NA_integer_

#     # Remove any metrics that are NA
#     # This can happen if any derivative metrics
#     # are calculated from qc inputs not provided
#     # in above lines.
#     for (colnm in names(qc_metrics)) {
#         is_all_na = all(is.na(qc_metrics[[colnm]]))
#         if (is_all_na) qc_metrics[[colnm]] = NULL
#     }

#     # qc_metrics[, `:=`(
#     #     m_reads = total_reads / 1e6,
#     #     m_reads_mapped = pf_reads_aligned / 1e6,
#     #     percent_optical_duplication = read_pair_optical_duplicates / read_pairs_examined,
#     #     percent_aligned = pf_aligned_bases / (total_reads * mean_read_length),
#     #     percent_optical_dups_of_dups = read_pair_optical_duplicates / read_pair_duplicates
#     # )]
    
#     return(as.list(qc_metrics))
# }


# list_of_qc = list(
#     dup_rate = "estimate_library_complexity",
#     tumor_cov = "tumor_wgs_metrics",
#     normal_cov = "normal_wgs_metrics",
#     insert_size = "insert_size_metrics",
#     alignment_summary = "alignment_summary_metrics"
# )

#' QC Metrics
#' 
#' Process QC metrics
#'
#' @param estimate_library_complexity Path to the estimate_library_complexity_metrics file
#' @param alignment_summary_metrics Path to the alignment_summary_metrics file
#' @param insert_size_metrics Path to the insert_size_metrics file
#' @param wgs_metrics Path to the wgs_metrics file
#' @param pair Sample pair identifier
#' @return A list containing processed QC metrics
process_qc_metrics <- function(
    estimate_library_complexity,
    alignment_summary_metrics,
    insert_size_metrics,
    tumor_wgs_metrics,
    normal_wgs_metrics,
    pair
) {

    # coalesce_cols = function(cohorttuple, cols) {
    #     lst = list()
    #     for (col in cols) {
    #         if (!exists(col, as.list(cohorttuple))) next
    #         lst = c(lst, list(cohorttuple[[col]]))
    #     }
    #     coalesced = do.call(
    #         Skilift:::coalesce,
    #         lst            
    #     )
    #     return(coalesced)
    # }

    # dup_path = coalesce_cols(cohorttuple, c(list_of_qc$dup_rate))
    # cov_tumor_path = coalesce_cols(cohorttuple, c(list_of_qc$tumor_cov))
    # cov_normal_path = coalesce_cols(cohorttuple, c(list_of_qc$normal_cov))
    # insert_path = coalesce_cols(cohorttuple, c(list_of_qc$insert_size))
    # aln_path = coalesce_cols(cohorttuple, c(list_of_qc$alignment_summary))
    # pair = cohorttuple$pair

    test_file_is_present = function(x) {
        (
            !is.null(x)
            && is.character(x)
            && NROW(x) == 1
            && file.exists(x)
        )
    }

    get_metrics = function(path, cols, pair = "pair") {
        nr = NROW(path)
        is_character = is.character(path)
        is_any_na = any(is.na(path))
        is_invalid = is_character && (! nr == 1 || is_any_na)
        if (is_invalid) stop(path, ": invalid path!")
        if (!test_file_is_present(path)) return(data.table::data.table(pair = pair))
        fcon = file(path, "r")
        txt = character(0)
        l = readLines(fcon, 1)
        is_comment = startsWith(l, "#")
        nr = NROW(l)
        is_empty = nr == 0 || !nzchar(l)
        while (is_comment && !is_empty) {
            l = readLines(fcon, 1)
            nr = NROW(l)
            is_empty = nr == 0 || !nzchar(l)
            is_comment = startsWith(l, "#")
        }
        txt = c(txt, l)
        while (!is_comment && !is_empty) {
            l = readLines(fcon, 1)
            txt = c(txt, l)
            nr = NROW(l)
            is_empty = nr == 0 || !nzchar(l)
            is_comment = startsWith(l, "#")
        }
        tbl = setDT(read.table(text = txt, header = TRUE, sep = "\t"))
        extract_metrics(qc_data = tbl, metrics = cols, pair = pair)
    }

    subset_which = function(obj, ix) {
        nr = NROW(ix)
        if (nr == 0) return(obj) else return(obj[ix])
    }


    # Define metric mappings for each file type
    complexity_metrics_cols <- c(
        read_pairs_examined = "READ_PAIRS_EXAMINED",
        read_pair_duplicates = "READ_PAIR_DUPLICATES",
        read_pair_optical_duplicates = "READ_PAIR_OPTICAL_DUPLICATES",
        percent_duplication = "PERCENT_DUPLICATION"
    )
    
    alignment_metrics_cols <- c(
        CATEGORY = "CATEGORY",
        total_reads = "TOTAL_READS",
        pf_reads_aligned = "PF_READS_ALIGNED",
        pf_aligned_bases = "PF_ALIGNED_BASES",
        mean_read_length = "MEAN_READ_LENGTH"
    )
    
    insert_metrics_cols <- c(
        PAIR_ORIENTATION = "PAIR_ORIENTATION",
        # median_insert_size = "MEDIAN_INSERT_SIZE"
        insert_size = "MEDIAN_INSERT_SIZE"
    )
    
    tumor_wgs_metrics_cols <- c(
        # median_coverage = "MEAN_COVERAGE"
        tumor_median_coverage = "MEDIAN_COVERAGE",
        # pct_30x = "PCT_30X",
        greater_than_or_equal_to_30x = "PCT_30X",
        # pct_50x = "PCT_50X"
        greater_than_or_equal_to_50x = "PCT_50X",
        fraction_excluded = "PCT_EXC_TOTAL"
    )

    normal_wgs_metrics_cols <- c(
        normal_median_coverage = "MEDIAN_COVERAGE",
        fraction_excluded = "PCT_EXC_TOTAL"
    )
    
    qc_dup = get_metrics(estimate_library_complexity, complexity_metrics_cols, pair)

    qc_aln = get_metrics(alignment_summary_metrics, alignment_metrics_cols, pair)
    ix = which(!qc_aln$CATEGORY=="PAIR")
    qc_aln = subset_which(qc_aln, -ix)
    qc_aln$fraction_of_reads_aligned = qc_aln$pf_reads_aligned / qc_aln$total_reads
    qc_aln$CATEGORY = NULL

    qc_insert = get_metrics(insert_size_metrics, insert_metrics_cols, pair)
    ix = which(!qc_insert$PAIR_ORIENTATION=="FR")
    qc_insert = subset_which(qc_insert, -ix)

    qc_cov_tumor = get_metrics(tumor_wgs_metrics, tumor_wgs_metrics_cols, pair)
    qc_cov_tumor$tumor_median_coverage = round(
        qc_cov_tumor$tumor_median_coverage / (1 - qc_cov_tumor$fraction_excluded)
    )
    qc_cov_tumor$fraction_excluded = NULL

    qc_cov_normal = get_metrics(normal_wgs_metrics, normal_wgs_metrics_cols, pair)
    qc_cov_normal$normal_median_coverage = round(
        qc_cov_normal$normal_median_coverage / (1 - qc_cov_normal$fraction_excluded)
    )
    qc_cov_normal$fraction_excluded = NULL

    # Read and extract metrics from each file
    
    # c("pair", "total_reads", "pf_reads_aligned", "pf_aligned_bases", 
    # "mean_read_length", "median_insert_size", "median_coverage", 
    # "pct_30x", "pct_50x", "percent_optical_duplication", 
    # "percent_aligned", "percent_optical_dups_of_dups")
    # Merge all metrics on pair
    lst_to_merge = list(
        qc_dup, 
        qc_aln, 
        qc_insert, 
        qc_cov_tumor,
        qc_cov_normal
    )
    qc_metrics <- Reduce(function(x, y) {
        data.table::merge.data.table(
            x, y, by = "pair", 
            all.x = TRUE, 
            all.y = TRUE,
            suffixes = c("_x", "_y")
        )
    }, lst_to_merge)
    
    # Calculate derivative metrics
    # Separating out and writing long way for robustness to
    # missing data.
    qc_metrics$m_reads = qc_metrics$total_reads / 1e6
    qc_metrics$m_reads_mapped = qc_metrics$pf_reads_aligned / 1e6
    qc_metrics$percent_optical_duplication = (
        qc_metrics$read_pair_optical_duplicates /
        qc_metrics$read_pairs_examined
    )
    qc_metrics$percent_aligned = (
        qc_metrics$pf_aligned_bases / 
        ( qc_metrics$total_reads * qc_metrics$mean_read_length )
    )
    qc_metrics$percent_optical_dups_of_dups = (
        qc_metrics$read_pair_optical_duplicates / 
        qc_metrics$read_pair_duplicates
    )
    # # FIXME, need to account for tumor and normal
    # qc_metrics$normal_median_coverage = NA_integer_

    # Remove any metrics that are NA
    # This can happen if any derivative metrics
    # are calculated from qc inputs not provided
    # in above lines.
    for (colnm in names(qc_metrics)) {
        is_all_na = all(is.na(qc_metrics[[colnm]]))
        if (is_all_na) qc_metrics[[colnm]] = NULL
    }

    # qc_metrics[, `:=`(
    #     m_reads = total_reads / 1e6,
    #     m_reads_mapped = pf_reads_aligned / 1e6,
    #     percent_optical_duplication = read_pair_optical_duplicates / read_pairs_examined,
    #     percent_aligned = pf_aligned_bases / (total_reads * mean_read_length),
    #     percent_optical_dups_of_dups = read_pair_optical_duplicates / read_pair_duplicates
    # )]
    
    return(as.list(qc_metrics))
}



#' QC Flags
#'
#' Parse QC flags into strings
#'
#' QC metrics from picard need to be parsed based on coverage, insert size
#' total number of reads, duplicate rate.
#' The strings should be parsable into a form
#' digested in gOS and shown as a single "PASS"/Checkmark", "Warning", or "Fail". 
#' The actual metrics should show up on hover.
#' @export
process_qc_flag = function(
    metadata,
    qc_flag_thresholds
) {

    if (is.null(qc_flag_thresholds)) {
        qc_flag_thresholds = Skilift:::qc_flag_thresholds
    }

    check_field = function(list_like, field) {
        val1 = list_like[[field]]
        val2 = list_like$coverage_qc[[field]]
        val3 = list_like$coverage_qc[[1]][[field]]
        val = val1
        if (is.null(val)) val = val2
        if (is.null(val)) val = val3
        if (is.null(val)) return(NA)
        return(val)
    }

    flags_lst = list()
    ops_oppo = c(
        ">=" = "<",
        "<=" = ">",
        ">" = "<=",
        "<" = ">=",
        "==" = "!=",
        "!=" = "=="
    )

    for (tuple in qc_flag_thresholds) {
        flag_title = tuple[[1]]
        field = tuple[[2]]
        fun_comparator = tuple[[3]]
        fun_comparator_string = gsub("\"", "", as.character(substitute(quote(fun_comparator))))
        fun_comparator_string = gsub(".Primitive", "", fun_comparator_string, perl = TRUE)
        fun_comparator_string = gsub("[\\(\\)]", "", fun_comparator_string, perl = TRUE)
        fun_comparator_string = fun_comparator_string[!fun_comparator_string %in% c("quote", ".Primitive")]
        if (NROW(fun_comparator_string) != 1) stop("wtf is going on with the comparator string")
        value = tuple[[4]]
        transformed_name = tuple[[5]]
        flag_message = tuple[[6]]
        data_value = check_field(metadata, field)
        is_na = any(is.na(data_value))
        if (is_na) {
            flag_title = "UNKNOWN"
            # flags_lst = c(flags_lst, list(paste("UNKNOWN:", transformed_name)))
            # next
        }
        is_flagged = all(fun_comparator(data_value, value))
        is_flagged = is_flagged && all(!is.na(is_flagged))
        op_to_show = fun_comparator_string
        if (!is_flagged) {
            flag_title = "PASS"
            op_to_show = ops_oppo[fun_comparator_string]
        }
        flag_shown_value = paste(
            op_to_show,
            " ",
            flag_message,
            sep = ""
        )
        key_json = flag_title
        value_json = paste(
            transformed_name, 
            " (", 
            signif(data_value, 3), 
            ")", " ",
            # flag_message,
            flag_shown_value,
            sep = ""
        )
        if (identical(flag_title, "UNKNOWN")) {
            value_json = transformed_name
        }
        value_json = trimws(value_json)
        ## record_json = list(
        ##     paste(
        ##         flag_title, 
        ##         ": ", 
        ##         transformed_name, 
        ##         " (", 
        ##         signif(data_value, 3), 
        ##         ")", " ",
        ##         flag_message,
        ##         sep = ""
        ##     )
        ## )
        record_json = list(list("code" = key_json, "title" = value_json))
        ## names(record_json)[names(record_json) == "key_field___rename"] = key_json
        ## names(record_json)[names(record_json) == "value_field___rename"] = value_json
        flags_lst = c(
            flags_lst, 
            record_json
        )
    }

    ## do.call(function(...) paste(..., collapse = "\n"), flags_lst)
    # metadata$qc_flag = paste(trimws(unlist(flags_lst)), collapse = "\n")
    # browser()
    metadata$qcMetrics = list(flags_lst)
    return(metadata)
}

#' @name add_coverage_metrics
#' @title Add Coverage Metrics
#' @description
#' Adds coverage metrics to the metadata based on tumor coverage and QC metrics.
#'
#' @param metadata A data.table containing metadata.
#' @param tumor_coverage Coverage data for the tumor.
#' @param foreground_col_name Name of the column in the coverage data to use for foreground coverage
#' @param estimate_library_complexity Path to library complexity metrics file
#' @param alignment_summary_metrics Path to alignment summary metrics file
#' @param insert_size_metrics Path to insert size metrics file
#' @param wgs_metrics Path to WGS metrics file
#' @return Updated metadata with coverage metrics added.
add_coverage_metrics <- function(
    metadata,
    tumor_coverage = NULL,
    foreground_col_name = "foreground",
    estimate_library_complexity = NULL,
    alignment_summary_metrics = NULL,
    insert_size_metrics = NULL,
    tumor_wgs_metrics = NULL,
    normal_wgs_metrics = NULL
) {
    coverage_variance <- NULL
    if (!is.null(tumor_coverage)) {
        foreground = as.data.table(readRDS(tumor_coverage))[[foreground_col_name]]
        cov_var <- dlrs(foreground) /100
        coverage_variance <- list(coverage_variance = cov_var)
        metadata$coverage_qc <- list(list(coverage_variance))
    }
    
    processed_metrics <- NULL
    if (
        !is.null(estimate_library_complexity) ||
        !is.null(alignment_summary_metrics) ||
        !is.null(insert_size_metrics) ||
        !is.null(tumor_wgs_metrics) ||
        !is.null(normal_wgs_metrics)
        ) {
        
        # processed_metrics <- process_qc_metrics2(
        #     estimate_library_complexity,
        #     alignment_summary_metrics,
        #     insert_size_metrics,
        #     tumor_wgs_metrics,
        #     normal_wgs_metrics,
        #     metadata$pair
        # )
        processed_metrics <- process_qc_metrics(
            estimate_library_complexity,
            alignment_summary_metrics,
            insert_size_metrics,
            tumor_wgs_metrics,
            normal_wgs_metrics,
            metadata$pair
        )
        
        metadata$coverage_qc <- list(as.list(c(processed_metrics, coverage_variance)))
        ## FIXME: hardcoding this for now.
        ## median and normal coverages are at top level..
        metadata$tumor_median_coverage = processed_metrics$tumor_median_coverage
        metadata$normal_median_coverage = processed_metrics$normal_median_coverage
    }

    return(metadata)
}

#' @name add_coverage_metrics
#' @title Add Coverage Metrics
#' @description
#' Adds coverage metrics to the metadata based on tumor coverage and QC metrics.
#'
#' @param metadata A data.table containing metadata.
#' @param tumor_coverage Coverage data for the tumor.
#' @param foreground_col_name Name of the column in the coverage data to use for foreground coverage
#' @param estimate_library_complexity Path to library complexity metrics file
#' @param alignment_summary_metrics Path to alignment summary metrics file
#' @param insert_size_metrics Path to insert size metrics file
#' @param wgs_metrics Path to WGS metrics file
#' @return Updated metadata with coverage metrics added.
add_conpair <- function(
    metadata,
    conpair_contamination = NULL,
    conpair_concordance = NULL
    ) {

    normal_conpair_contamination_value = NA_real_
    tumor_conpair_contamination_value = NA_real_
    if (!is.null(conpair_contamination)) {
        conpair_metrics = readLines(conpair_contamination)
        tumor_conpair_contamination_value = gsub(".*: ", "", conpair_metrics[2])
        tumor_conpair_contamination_value = as.numeric(gsub("%", "", tumor_conpair_contamination_value)) / 100
        normal_conpair_contamination_value = gsub(".*: ", "", conpair_metrics[1])
        normal_conpair_contamination_value = as.numeric(gsub("%", "", normal_conpair_contamination_value)) / 100
    }



    conpair_concordance_metric = NA_real_
    if (!is.null(conpair_concordance)) {
        conpair_metrics = readLines(conpair_concordance)
        conpair_concordance_metric = as.numeric(conpair_metrics[1])
    }

    metadata$tumor_conpair_contamination_metric = tumor_conpair_contamination_value
    metadata$normal_conpair_contamination_metric = normal_conpair_contamination_value
    metadata$conpair_concordance_metric = conpair_concordance_metric
    return(metadata)
}

#' @name vcf_count
#' @title vcf_count
#' @description
#' takes in a vcf and returns a data.table with total count and count of variants with normal vaf greater than 0
#'
#' @param vcf_path Path to the VCF file
#' @param genome Reference genome name (e.g., "hg19", "hg38")
#' @return data.table
#' @export
#' @author Shihab Dider, Tanubrata Dey
vcf_count <- function(
    vcf_path,
    genome) {
    if (!file.exists(vcf_path)) {
        stop("VCF file does not exist.")
    }
    is_len_one = NROW(vcf_path) == 1
    is_character = is.character(vcf_path)
    is_na = is_len_one && is_character && (is.na(vcf_path) || vcf_path %in% c("NA", "na"))
    is_vcf = is_len_one && is_character && !is_na && grepl("vcf(.gz|.bgz)?$", vcf_path)
    is_bcf = is_len_one && is_character && !is_na && grepl("bcf(.gz|.bgz)?$", vcf_path)
    is_valid_path = is_vcf || is_bcf
    if (!is_valid_path) stop("either bcf or vcf path must be provided to vcf_count()")
    if (is_bcf) {
        temp_vcf = tempfile(fileext = ".vcf")
        on.exit(unlink(temp_vcf, force = TRUE))
        system2("bcftools", c("view", vcf_path, "-Ov", "-o", temp_vcf))
        vcf_path = temp_vcf
    }
    vcf <- readVcf(vcf_path, genome)

    # Filter for PASS variants
    pass_variants <- rowRanges(vcf)$FILTER == "PASS"
    vcf <- vcf[pass_variants, ]

    snv_count <- length(vcf)

    return(
        data.table(
            category = c("snv_count", "snv_count_normal_vaf_greater0"),
            counts = c(snv_count, NA)
        )
    )
}
    
#' @name add_variant_counts
#' @title Add Variant Counts
#' @description
#' Adds variant counts to the metadata based on somatic and germline SNVs.
#'
#' @param metadata A data.table containing metadata.
#' @param somatic_snvs Path to somatic SNV VCF file
#' @param genome The genome reference used.
#' @return Updated metadata with variant counts added.
add_variant_counts <- function(
    metadata,
    somatic_snvs = NULL,
    genome = "hg19"
) {
    if (!is.null(somatic_snvs)) {
        is_path_character = is.character(somatic_snvs)
        is_length_one = NROW(somatic_snvs) == 1
        is_snvs_exists = is_path_character && is_length_one && file.exists(somatic_snvs)
        is_rds = is_snvs_exists && grepl("rds$", somatic_snvs)
        is_vcf = is_snvs_exists && grepl("(v|b)cf(.gz|.bgz)?$", somatic_snvs)
        is_txt = is_snvs_exists && grepl("(txt|maf|(t|c)sv)(.gz|.bgz)?$", somatic_snvs)
        is_other = is_txt || is_rds
        snv_counts_dt = data.table()
        if (is_vcf) {
            snv_counts_dt <- vcf_count(somatic_snvs, genome = genome)
        }
        if (is_rds) snvs = readRDS(somatic_snvs)
        if (is_txt) snvs = fread(somatic_snvs)
        if (is_other) {
            snv_counts_dt = data.table(
                category = c("snv_count", "snv_count_normal_vaf_greater0"),
                counts = c(NROW(snvs), NA_integer_)
            )
        }
        
        metadata$snv_count <- snv_counts_dt[category == "snv_count", ]$counts
        ## count up filtered variants in VCF 

        metadata$snv_count_normal_vaf_greater0 <- snv_counts_dt[category == "snv_count_normal_vaf_greater0", ]$counts
    }
    return(metadata)
}

#' @name add_sv_counts
#' @title Add Structural Variant Counts
#' @description Adds junction, loose end, and total SV counts from JaBbA graph
#' @param meta_dt A data.table containing metadata
#' @param jabba_gg Path to JaBbA graph RDS file
#' @return Updated metadata with SV counts
add_sv_counts <- function(metadata, jabba_gg = NULL) {
    if (is.null(jabba_gg)) {
        return(metadata)
    }
    
    gg <- process_jabba(jabba_gg)
    
    # Count junctions
    if (nrow(gg$junctions$dt) > 0) {
        metadata$junction_count <- nrow(gg$junctions$dt[type != "REF", ])
    } else {
        warning("No junctions found in JaBbA graph, skipping SV counts...")
        metadata$junction_count <- 0
    }
    
    # Count loose ends
    if (length(gg$loose) > 0) {
        metadata$loose_count <- nrow(as.data.table(gg$loose)[terminal == FALSE, ])
    } else {
        warning("No loose ends found in JaBbA graph, skipping loose counts...")
        metadata$loose_count <- 0
    }
    
    # Calculate total SV count
    metadata[, sv_count := (junction_count)]
    
    return(metadata)
}

#' @name add_purity_ploidy
#' @title Add Purity and Ploidy Metrics
#' @description Adds purity and ploidy information from JaBbA graph
#' @param metadata A data.table containing metadata
#' @param jabba_gg Path to JaBbA graph RDS file
#' @return Updated metadata with purity and ploidy information
add_purity_ploidy <- function(metadata, purple_pp_bestFit = NULL, jabba_gg = NULL, tumor_coverage = NULL) {
    is_null_or_na_jabba_gg = is.null(jabba_gg) || any(is.na(jabba_gg))
    is_null_or_na_purple = is.null(purple_pp_bestFit) || any(is.na(purple_pp_bestFit))
    is_jabba_gg_and_purple_absent = is_null_or_na_jabba_gg && is_null_or_na_purple

    if (is_jabba_gg_and_purple_absent) {
        return(metadata)
    }
    
    if (!is_null_or_na_jabba_gg) {
        gg <- process_jabba(jabba_gg)

        purity = base::get("purity", gg$meta) # Errors out if not found
        ploidy = base::get("ploidy", gg$meta) # Errors out if not found
    } else if (!is_null_or_na_purple) {
        purple_best_fit = fread(purple_pp_bestFit)
        purity = purple_best_fit$purity
        ploidy = purple_best_fit$ploidy
    }

    metadata$purity <- purity
    metadata$ploidy <- ploidy
    metadata$beta = purity / (purity * ploidy + 2*(1 - purity)) # from Multiplicity
    metadata$gamma = 2*(1 - purity) / (purity * ploidy + 2*(1 - purity)) # from Multiplicity
    
    return(metadata)
}

#' @name add_loh
#' @title Add Loss of Heterozygosity Metrics
#' @description Adds LOH metrics from JaBbA graph
#' @param metadata A data.table containing metadata
#' @param jabba_gg Path to JaBbA graph RDS file
#' @param seqnames_loh Vector of sequence names to consider for LOH
#' @return Updated metadata with LOH metrics
add_loh <- function(
    metadata,
    jabba_gg = NULL,
    seqnames_loh = c(1:22)
) {
    if (is.null(jabba_gg) || is.null(seqnames_loh)) {
        return(metadata)
    }
    
    gg <- process_jabba(jabba_gg)
    nodes.dt <- gg$nodes$dt
    nodes.dt[, seqnames := gsub("chr", "", seqnames)]
    nodes.dt <- nodes.dt[seqnames %in% seqnames_loh]
    totalseglen <- nodes.dt$width %>% sum()
    
    if ("cn.low" %in% names(nodes.dt)) {
        lohsegs <- nodes.dt[cn.low == 0, ] %>%
            .[cn.high > 0] %>%
            .$width %>%
            sum()
        loh_frc <- lohsegs / totalseglen
        metadata[, `:=`(
            loh_fraction = loh_frc,
            loh_seglen = lohsegs,
            loh_total_genome = totalseglen
        )]
    } else {
        metadata$loh_fraction <- "Not Allelic Jabba"
    }
    
    return(metadata)
}

#' @name add_fga
#' @title Add Fraction Genome Altered (FGA)
#' @param metadata A data.table containing metadata
#' @param jabba_gg Path to JaBbA graph RDS file
#' @param seqnames Vector of sequence names to consider
#' @return Updated metadata with LOH metrics
add_fga <- function(
    metadata,
    jabba_gg = NULL,
    seqnames = c(1:22)
) {
    if (is.null(jabba_gg) || is.null(seqnames)) {
        return(metadata)
    }
    
    gg <- process_jabba(jabba_gg)
    nodes.dt <- gg$nodes$dt
    nodes.dt[, seqnames := gsub("chr", "", seqnames)]
    nodes.dt <- base::subset(nodes.dt, subset = nodes.dt$seqnames %in% seqnames)
    totalseglen <- nodes.dt$width %>% sum()

    
    fga_width = sum(nodes.dt[cn != 2]$width)
    fga <- fga_width / totalseglen
    metadata$loh_fraction = fga ## FIXME: This is FGA, this is to maintain compatibility with gOS frontend
    
    return(metadata)
}

#' @name add_genome_length
#' @title Add Total Genome Length
#' @description Adds total genome length from JaBbA graph
#' @param metadata A data.table containing metadata
#' @param jabba_gg Path to JaBbA graph RDS file
#' @param seqnames_genome_width_or_genome_length Vector of sequence names to include in genome length or genome length as a numeric
#' @return Updated metadata with genome length information
add_genome_length <- function(
    metadata,
    jabba_gg = NULL,
    seqnames_genome_width_or_genome_length = c(1:22, "X", "Y")
) {

    is_null_jabba = is.null(jabba_gg)

    is_genome_length_already_provided = is.numeric(seqnames_genome_width_or_genome_length) && NROW(seqnames_genome_width_or_genome_length) == 1

    # handle targeted panels/whole exome
    if (is_genome_length_already_provided) {
        metadata$total_genome_length <- seqnames_genome_width_or_genome_length
        return(metadata)
    }

    # otherwise, handle genome-wide
    is_no_jabba_or_genome_length_provided = is_null_jabba && is.null(seqnames_genome_width_or_genome_length)
    if (is_no_jabba_or_genome_length_provided) {
        return(metadata)
    }

    if (is_null_jabba) {
         ## FIXME: hardcoding to hg19!!
        genome_length_hg19 = 3095677412
        message("NO JABBA OR GENOME LENGTH PROVIDED, hardcoding to hg19: ", genome_length_hg19)
        metadata$total_genome_length = genome_length_hg19
        return(metadata)
    }

    gg <- process_jabba(jabba_gg)
    nodes.gr <- gg$nodes$gr
    seqlengths.dt <- suppressWarnings(
        as.data.table(
            seqinfo(nodes.gr),
            keep.rownames = "seqnames"
        )
    )
    seqlengths.dt[, seqnames := gsub("chr", "", seqnames)]
    if (!is.null(seqnames_genome_width_or_genome_length)) {
        seqlengths.dt <- seqlengths.dt[seqnames %in% seqnames_genome_width_or_genome_length, ]
    }
    
    metadata$total_genome_length <- sum(seqlengths.dt$seqlengths)
    
    return(metadata)
}

#' @name add_sv_types
#' @title Add Structural Variant Types
#' @description Adds counts of different SV types from JaBbA graph and complex events
#' @param metadata A data.table containing metadata
#' @param jabba_gg Path to JaBbA graph RDS file
#' @param complex Path to complex events RDS file
#' @return Updated metadata with SV type counts
add_sv_types <- function(metadata, jabba_gg = NULL, complex = NULL) {
    sv_like_types_count <- NULL
    if (!is.null(jabba_gg)) {
        gg <- process_jabba(jabba_gg)
        if (!is.null(gg$edges$dt) && nrow(gg$edges$dt) > 0) {
            sv_like_types_count <- table(gg$edges$dt[type == "ALT" & class != "REF"]$class)
            metadata$sv_types_count <- list(list(list(sv_like_types_count)))
        } else {
            warning("No edges found in JaBbA graph, skipping SV-like types count...")
        }
    }
    
    qrp_counts <- data.table(qrpmin = 0, qrpmix = 0, qrppos = 0)
    
    sv_types_count <- NULL
    if (!is.null(complex)) {
        complex_data <- process_jabba(complex)
        if (!is.null(complex_data$meta$events$type)) {
            sv_types_count <- table(complex_data$meta$events$type)
            
            # Update QRP counts
            qrp_types <- c("qrpmin", "qrpmix", "qrppos")
            for (qrp_type in qrp_types) {
                if (qrp_type %in% names(sv_types_count)) {
                    qrp_counts[[qrp_type]] <- sv_types_count[[qrp_type]]
                }
            }
            
        }
    } else {
        warning("Complex events and JaBbA graph not found as inputs, skipping SV types count...")
    }
    
    metadata$sv_types_count <- list(as.list(c(sv_types_count, sv_like_types_count)))
    metadata$hrd <- list(as.list(qrp_counts))
    return(metadata)
}

#' @name add_coverage_parameters
#' @title Add Coverage Parameters
#' @description Adds coverage parameters to the metadata based on tumor coverage
#' @param metadata A data.table containing metadata
#' @param tumor_coverage Coverage data for the tumor
#' @param field The field to use for coverage calculation
#' @return Updated metadata with coverage parameters added
add_coverage_parameters <- function(metadata, tumor_coverage, field = "foreground") {
    if (!is.null(tumor_coverage)) {
        if (is.null(metadata$purity) || is.null(metadata$ploidy)) {
            warning("Purity and ploidy not found in metadata, cov_slope and cov_intercept will not be calculated")
        }
        cov <- tumor_coverage %>% readRDS()
		coverage_values = base::get0(field, as.environment(as.list(mcols(cov))), ifnotfound = NULL)
		lst_cov_bool = Skilift::test_coverage_normalized(coverage_values)
		is_cov_likely_normalized = lst_cov_bool$is_cov_likely_normalized
		is_cov_near_one = lst_cov_bool$is_cov_near_one
		if (!is_cov_likely_normalized && !is_cov_near_one) {
			message("Assuming coverage is in read coverage per bin and paired end, 151 bp reads, rescaling")
			mcols(cov)[[field]] = coverage_values * PAIRED_READS_FACTOR * READ_LENGTH / width(cov)
		} else {
			message("Assuming coverage is mean-normalized, ignoring rescaling to base coverage")
		}
        # mcols(cov)[[field]] <- mcols(cov)[, field] * 2 * 151 / width(cov)
        rel2abs.cov <- skitools::rel2abs(cov,
            field = field,
            purity = metadata$purity,
            ploidy = metadata$ploidy,
            return.params = TRUE
        )
        metadata$cov_slope <- rel2abs.cov[1] %>% unname()
        metadata$cov_intercept <- rel2abs.cov[2] %>% unname()
    }
    return(metadata)
}

#' @name add_het_pileups_parameters
#' @title Add Heterozygous Pileups Parameters
#' @description Adds slope and intercept parameters calculated from heterozygous pileups
#' @param metadata A data.table containing metadata
#' @param het_pileups_wgs Path to heterozygous pileups WGS data
#' @return Updated metadata with heterozygous pileups parameters
add_het_pileups_parameters <- function(metadata, het_pileups) {
    if (!is.null(het_pileups)) {
        hets.read <- grab.hets(het_pileups) %>% gr2dt()
        hets.read <- dt2gr(hets.read[, .(count = sum(count)), by = c("seqnames", "start", "end")])
        if (is.null(metadata$purity) || is.null(metadata$ploidy)) {
            warning("Purity and ploidy not found in metadata, het_slope and het_intercept will not be calculated")
        }
        rel2abs.hets <- skitools::rel2abs(hets.read,
            field = "count",
            purity = metadata$purity,
            ploidy = metadata$ploidy,
            return.params = TRUE
        )
        metadata$hets_slope <- rel2abs.hets[1] %>% unname()
        metadata$hets_intercept <- rel2abs.hets[2] %>% unname()
    }
    return(metadata)
}

#' @name add_tmb
#' @title Add Tumor Mutation Burden
#' @description
#' Calculates and adds tumor mutation burden (TMB) to the metadata based on SNV count and genome length
#'
#' @param metadata A data.table containing metadata
#' @param somatic_snvs Path to somatic SNV VCF file
#' @param jabba_gg Path to JaBbA graph RDS file
#' @param genome The genome reference used
#' @param seqnames_genome_width_or_genome_length Sequence names for genome width calculation
#' @return Updated metadata with TMB value added
add_tmb <- function(
    metadata,
    somatic_snvs = NULL,
    jabba_gg = NULL,
    genome = "hg19",
    seqnames_genome_width_or_genome_length = c(1:22, "X", "Y")
) {
    meta_dt <- copy(metadata)
    # First ensure we have snv_count
    if (is.null(meta_dt$snv_count) || is.na(meta_dt$snv_count)) {
        meta_dt <- add_variant_counts(meta_dt, somatic_snvs, genome)
    }
    
    # Then ensure we have total_genome_length
    if (is.null(meta_dt$total_genome_length) || is.na(meta_dt$total_genome_length)) {
        meta_dt <- add_genome_length(meta_dt, jabba_gg, seqnames_genome_width_or_genome_length)
    }
    
    
    # Calculate TMB if we have both required values
    is_tmb_computable <- !is.na(meta_dt$snv_count) && !is.na(meta_dt$total_genome_length) && !is.null(meta_dt$snv_count) && !is.null(meta_dt$total_genome_length)
    if (is_tmb_computable) {
        tmb_vals = meta_dt$snv_count / (as.numeric(meta_dt$total_genome_length) / 1e6)
        metadata[, tmb := tmb_vals]
        metadata[, tmb := round(tmb, digits = 3)]
    } else {
        warning("Cannot calculate TMB without both snv_count and total_genome_length")
    }
    
    return(metadata)
}

#' @name compute_signature_averages
#' @title Compute Signature Averages
#' @description
#' Computes signature averages from various signature files
#'
#' @param sig_file Path to signature file
#' @param sample Sample or pair name needed to subset to the correct sample
#' @param is_indel Boolean indicating if processing indel signatures
#' @param is_deconstruct_sigs Boolean indicating if using deconstructSigs format
#' @return List of signature averages
compute_signature_averages <- function(
    sig_file,
    is_indel = FALSE,
    is_deconstruct_sigs = FALSE
) {
    if (is_deconstruct_sigs) {
        sig_data <- readRDS(sig_file)
        weights <- as.data.table(sig_data$weights)
        weights <- t(weights)
        sigs.dt <- as.data.table(weights, keep.rownames = "Signature") %>% 
            setnames(., c("Signature", "weights"))
        sigs.dt[, Signature := gsub("Signature.", "SBS", Signature)]
        sigs.vect <- sigs.dt$weights
        names(sigs.vect) <- sigs.dt$Signature
        return(sigs.vect)
    } else {
        sig.dt <- fread(sig_file)
        if (nrow(sig.dt) == 0) {
            stop("No signatures found in file")
        }
        pair <- sig.dt$Samples[1]
        sig.dt[, pair := pair]
        sig.dt[, Samples := NULL]
        sig.dt_avg <- copy(sig.dt)
        sig.dt_avg[, pair := NULL]
        row_sum <- sig.dt_avg %>% rowSums()
        sigs.dt <- melt.data.table(sig.dt_avg, measure.vars = names(sig.dt_avg)) %>% 
            setnames(., c("signature", "value"))
        sigs.dt[, avg_value := value / row_sum]
        sigs.vect <- sigs.dt$avg_value
        names(sigs.vect) <- sigs.dt$signature
        sigs_counts.vect <- sigs.dt$value
        names(sigs_counts.vect) <- sigs.dt$signature
        
        if (is_indel) {
            return(list(
                indel_fraction = sigs.vect,
                indel_count = sigs_counts.vect
            ))
        } else {
            return(list(
                sbs_fraction = sigs.vect,
                sbs_count = sigs_counts.vect
            ))
        }
    }
}

#' @name add_signatures
#' @title Add Signatures
#' @description
#' Adds mutational signatures to the metadata based on various signature matrices.
#'
#' @param metadata A data.table containing metadata.
#' @param signatures_pair_name The name of the signature pair.
#' @param activities_sbs_signatures Activities of SBS signatures.
#' @param activities_indel_signatures Activities of indel signatures.
#' @param deconstructsigs_sbs_signatures DeconstructSigs SBS signatures.
#' @return Updated metadata with mutational signatures added.
add_signatures <- function(
    metadata,
    activities_sbs_signatures,
    activities_indel_signatures,
    deconstructsigs_sbs_signatures
) {
    # Initialize list columns with proper length
    n <- nrow(metadata)
    metadata[, `:=`(
        deconstructsigs_sbs_fraction = list(list()),
        deletionInsertion = list(list()),
        sigprofiler_indel_fraction = list(list()),
        sigprofiler_indel_count = list(list()),
        sigprofiler_sbs_fraction = list(list()),
        sigprofiler_sbs_count = list(list()),
        signatures = list(list())
    )]

    if (!is.null(activities_sbs_signatures)) {
        signatures <- compute_signature_averages(
            sig_file = activities_sbs_signatures,
            is_indel = FALSE,
            is_deconstruct_sigs = FALSE
        )

        metadata$sigprofiler_sbs_fraction <- list(as.list(signatures[["sbs_fraction"]]))
        metadata$sigprofiler_sbs_count <- list(as.list(signatures[["sbs_count"]]))
        metadata$signatures <- list(as.list(signatures[["sbs_fraction"]]))
    }

    if (!is.null(activities_indel_signatures)) {
        deletionInsertion <- compute_signature_averages(
            sig_file = activities_indel_signatures,
            is_indel = TRUE,
            is_deconstruct_sigs = FALSE
        )
        metadata$deletionInsertion <- list(as.list(deletionInsertion[["indel_fraction"]]))
        metadata$sigprofiler_indel_fraction <- list(as.list(deletionInsertion[["indel_fraction"]]))
        metadata$sigprofiler_indel_count <- list(as.list(deletionInsertion[["indel_count"]]))
    }

    if (!is.null(deconstructsigs_sbs_signatures)) {
        signatures <- compute_signature_averages(
            sig_file = deconstructsigs_sbs_signatures,
            is_indel = FALSE,
            is_deconstruct_sigs = TRUE
        )
        metadata$deconstructsigs_sbs_fraction <- list(as.list(signatures))
    }

    return(metadata)
}

#' @name add_hrd_scores
#' @title Add HRD Scores
#' @description
#' Adds homologous recombination deficiency (HRD) scores to the metadata.
#'
#' @param metadata A data.table containing metadata.
#' @param hrdetect HRDetect scores.
#' @param onenesstwoness Oneness and twoness scores.
#' @return Updated metadata with HRD scores added.
add_hrd_scores <- function(metadata, hrdetect, onenesstwoness) {
    if (!is.null(onenesstwoness)) {
        onetwo <- readRDS(onenesstwoness)
        hrd_values <- data.table(
            b1_2_score = onetwo$ot_scores[, "BRCA1"] + onetwo$ot_scores[, "BRCA2"],
            b1_score = onetwo$ot_scores[, "BRCA1"],
            b2_score = onetwo$ot_scores[, "BRCA2"],
            wt_score = onetwo$ot_scores[, "OTHER"],
            DUP_1kb_100kb = onetwo$expl_variables[, "DUP_1kb_100kb"],
            SNV3 = onetwo$expl_variables[, "SNV3"],
            SNV8 = onetwo$expl_variables[, "SNV8"],
            RS3 = onetwo$expl_variables[, "RS3"],
            RS5 = onetwo$expl_variables[, "RS5"],
            del_mh_prop = onetwo$expl_variables[, "del.mh.prop"],
            ihdel = onetwo$expl_variables[, "ihdel"],
            loh_score = onetwo$expl_variables[, "hrd"],
            qrppos = onetwo$expl_variables[, "qrppos"],
            qrpmin = onetwo$expl_variables[, "qrpmin"],
            qrpmix = onetwo$expl_variables[, "qrpmix"],
            qrdup = onetwo$expl_variables[, "qrdup"],
            qrdel = onetwo$expl_variables[, "qrdel"],
            tib = onetwo$expl_variables[, "tib"]
        )

        if (!is.null(hrdetect)) {
            hrd <- readRDS(hrdetect)
            hrd_values <- hrd_values[, hrd_score := hrd$hrdetect_output[1, "Probability"]]
        } else {
            warning("HRDetect scores not found, skipping HRD scores...")
        }

        metadata$hrd <- list(as.list(hrd_values))

    } else {
        warning("Oneness and twoness scores not found, skipping...")
    }
    
    return(metadata)
}

#' @name add_msisensor_score
#' @title Add MSIsensor Score
#' @description
#' Adds MSIsensor score to the metadata.    
#' 
#' @param metadata A data.table containing metadata.
#' @param msisensor_pro Path to MSIsensor profile file.
#' @return Updated metadata with MSIsensor score added.
add_msisensor_score <- function(metadata, msisensorpro) {
    tryCatch({
        if (!is.null(msisensorpro)) {
            msisensor_data <- fread(msisensorpro)
            if (nrow(msisensor_data) != 0) {
                score <- msisensor_data[[3]][1]
                label.msi <- ifelse(score < 10, "MSS",
                                    ifelse(score < 20, "MSI-Low", "MSI-High"))
                dt <- data.table(
                    score = score / 100,
                    n_unstable = msisensor_data[[2]][1],
                    n_evaluated = msisensor_data[[1]][1],
                    label = label.msi
                )
                metadata$msisensor <- list(as.list(dt))
            } else {
                warning("MSIsensor profile is empty, skipping MSIsensor score...")
            }
        } else {
            warning("MSIsensor profile not provided, skipping MSIsensor score...")
        }
    }, error = function(e) {
        warning(sprintf("Error processing MSIsensor score: %s", e$message))
    })
    return(metadata)
}

#' @title Create Metadata for a Sample
#' @description
#' Creates a comprehensive metadata object for a single sample pair by aggregating various data inputs.
#'
#' @param pair The sample pair identifier.
#' @param tumor_type The type of tumor.
#' @param tumor_details Details about the tumor.
#' @param disease The disease associated with the sample.
#' @param primary_site The primary site of the tumor.
#' @param inferred_sex The inferred sex of the sample.
#' @param jabba_gg A file path to the jabba_gg data.
#' @param events Structural variant events.
#' @param somatic_snvs Somatic single nucleotide variants.
#' @param germline_snvs Germline single nucleotide variants.
#' @param tumor_coverage Coverage data for the tumor.
#' @param estimate_library_complexity Path to the estimate_library_complexity_metrics file.
#' @param alignment_summary_metrics Path to the alignment_summary_metrics file.
#' @param insert_size_metrics Path to the insert_size_metrics file.
#' @param wgs_metrics Path to the wgs_metrics file.
#' @param het_pileups Heterozygous pileups data.
#' @param signatures_pair_name The name of the signature pair.
#' @param matrix_indel_signatures Matrix of indel signatures.
#' @param matrix_sbs_signatures Matrix of SBS signatures.
#' @param activities_sbs_signatures Activities of SBS signatures.
#' @param hrdetect HRDetect scores.
#' @param onenesstwoness Oneness and twoness scores.
#' @param msisensorpro MSIsensor profile file.
#' @param genome The genome reference used.
#' @param seqnames_loh Sequence names for loss of heterozygosity.
#' @param seqnames_genome_width_or_genome_length Sequence names and genome width in list or genome length as a numeric
#' @return A data.table containing the metadata for a single sample pair.
#' @export
create_metadata <- function(
    pair,
    tumor_type = NULL,
    tumor_details = NULL,
    disease = NULL, 
    primary_site = NULL,
    inferred_sex = NULL,
    purple_pp_bestFit = NULL,
    jabba_gg = NULL,
    events = NULL,
    somatic_snvs = NULL,
    germline_snvs = NULL,
    tumor_coverage = NULL,
    foreground_col_name = "foreground",
    estimate_library_complexity = NULL,
    alignment_summary_metrics = NULL,
    insert_size_metrics = NULL,
    tumor_wgs_metrics = NULL,
    normal_wgs_metrics = NULL,
    het_pileups = NULL,
    activities_indel_signatures = NULL,
    deconstructsigs_sbs_signatures = NULL,
    activities_sbs_signatures = NULL,
    hrdetect = NULL,
    onenesstwoness = NULL,
    msisensorpro = NULL,
    genome = "hg19",
    seqnames_loh = c(1:22),
    seqnames_autosomes = c(1:22),
    seqnames_genome_width_or_genome_length = c(1:22, "X", "Y"),
    denoised_coverage_field = "foreground",
    is_visible = TRUE,
	summary = NULL,
    conpair_contamination = NULL,
    conpair_concordance = NULL,
    cohort_type = NULL,
    qc_flags_config = NULL
) {
    # Initialize metadata with all possible columns
    metadata <- initialize_metadata_columns(pair)
    # change NA to NULL
    fix_entries = c("tumor_type", "tumor_details", "disease", "primary_site", "inferred_sex", "jabba_gg", "events", "somatic_snvs", "germline_snvs", "tumor_coverage", "estimate_library_complexity", "alignment_summary_metrics", "insert_size_metrics", "wgs_metrics", "het_pileups", "activities_indel_signatures", "deconstructsigs_sbs_signatures", "activities_sbs_signatures", "hrdetect", "onenesstwoness", "msisensorpro", "denoised_coverage_field", "summary", "conpair_contamination")
    for (x in fix_entries) {
        if (!exists(x) || is.null(get(x)) || is.na(get(x))) {
            assign(x, NULL)
        }
    }    

    # Add each component sequentially
    metadata <- add_basic_metadata(metadata, tumor_type, tumor_details, disease, primary_site)
    metadata <- add_sex_information(metadata, inferred_sex, jabba_gg, tumor_coverage)
    # Add coverage metrics
    metadata <- add_coverage_metrics(
        metadata = metadata,
        tumor_coverage = tumor_coverage,
        foreground_col_name = foreground_col_name,
        estimate_library_complexity = estimate_library_complexity,
        alignment_summary_metrics = alignment_summary_metrics,
        insert_size_metrics = insert_size_metrics,
        tumor_wgs_metrics = tumor_wgs_metrics,
        normal_wgs_metrics = normal_wgs_metrics
    )
    metadata <- add_variant_counts(metadata, somatic_snvs, genome)
    
    # New SV-related function calls
    metadata <- add_sv_counts(metadata, jabba_gg)
    metadata <- add_purity_ploidy(metadata, purple_pp_bestFit = purple_pp_bestFit, jabba_gg = jabba_gg, tumor_coverage = tumor_coverage)
    # metadata <- add_loh(metadata, jabba_gg, seqnames_loh)
    metadata <- add_fga(metadata, jabba_gg, seqnames_autosomes)
    metadata <- add_genome_length(metadata, jabba_gg, seqnames_genome_width_or_genome_length)
    metadata <- add_sv_types(metadata, jabba_gg, events)
    metadata <- add_coverage_parameters(metadata, tumor_coverage, denoised_coverage_field)
    metadata <- add_het_pileups_parameters(metadata, het_pileups)
    
    # Add TMB calculation
    if (!cohort_type == "heme")
        metadata <- add_tmb(metadata, somatic_snvs, jabba_gg, genome, seqnames_genome_width_or_genome_length)
    
    metadata <- add_signatures(
        metadata,
        activities_sbs_signatures,
        activities_indel_signatures,
        deconstructsigs_sbs_signatures
    )
    
    # Add HRD scores
    metadata <- add_hrd_scores(metadata, hrdetect, onenesstwoness)

    # Add MSIsensor score
    metadata <- add_msisensor_score(metadata, msisensorpro)
    metadata <- add_conpair(metadata = metadata, conpair_contamination = conpair_contamination, conpair_concordance = conpair_concordance)

    if (!as.logical(is_visible)) {
        metadata$visible <- FALSE
    }

    metadata = process_qc_flag(metadata, qc_flags_config)

	metadata$summary = summary
    
    return(metadata)
}

#' @name lift_metadata
#' @title lift_metadata
#' @description
#' Create metadata files for all samples in a cohort
#'
#' @param cohort Cohort object containing sample information
#' @param output_data_dir Base directory for output files
#' @param cores Number of cores for parallel processing (default: 1)
#' @param genome_length Genome length for the samples (for targeted panels or WES data)
#' @return None
#' @export
lift_metadata <- function(cohort, output_data_dir, cores = 1, genome_length = c(1:22, "X", "Y"), do_lift_datafiles_json = TRUE) {
    if (!inherits(cohort, "Cohort")) {
        stop("Input must be a Cohort object")
    }
    
    if (!dir.exists(output_data_dir)) {
        dir.create(output_data_dir, recursive = TRUE)
    }

    lift_inputs = pairify_cohort_inputs(cohort, tumor_normal_columns = c("wgs_metrics"), keep_remaining = TRUE, sep_cast = "_")

    jabba_column = Skilift::DEFAULT_JABBA(object = cohort)
    # Define all possible columns
    all_cols <- c(
        "pair", "tumor_type", "tumor_details", "disease", "primary_site", "inferred_sex",
        # "jabba_gg", 
		jabba_column,
		"events", "oncokb_snv", "somatic_snvs", "germline_snvs", "tumor_coverage",
        "estimate_library_complexity", "alignment_summary_metrics",
        "insert_size_metrics", "tumor_wgs_metrics", "normal_wgs_metrics",
        "het_pileups", "activities_sbs_signatures", "activities_indel_signatures",
        "hrdetect", "onenesstwoness", "msisensorpro", "string_summary"
    )
    
    # Check for required column
    if (!"pair" %in% names(lift_inputs)) {
        stop("Missing required column 'pair' in cohort")
    }
    
    # Warn about missing optional columns
    missing_cols <- all_cols[!all_cols %in% names(lift_inputs)]
    if (length(missing_cols) > 0) {
        warning("Missing optional columns in cohort: ", paste(missing_cols, collapse = ", "))
    }

    if(is.null(genome_length)) {
        warning("No genome length provided, assuming WGS data")
        genome_length <- c(1:22, "X", "Y")
    }
    
    cohort_type = cohort$type
    # Process each sample in parallel
    list_metadata = mclapply(seq_len(nrow(lift_inputs)), function(i) {
        row <- lift_inputs[i,]
        pair_dir <- file.path(output_data_dir, row$pair)
        
        if (!dir.exists(pair_dir)) {
            dir.create(pair_dir, recursive = TRUE)
        }
        
        out_file <- file.path(pair_dir, "metadata.json")

        # prefer oncokb_snv over somatic_snvs if available
        is_oncokb_present = !is.null(row$oncokb_snv) && !is.na(row$oncokb_snv)
        snvs_column = row$somatic_snvs
        if (is_oncokb_present) {
            snvs_column = row$oncokb_snv
        }
		
		inferred_sex_field = row$inferred_sex

		purple_qc_path_for_fread = row$purple_qc
		is_purple_qc_null = is.null(purple_qc_path_for_fread) 
		is_purple_pp_range_null = is.null(row$purple_pp_range)
		extracted_purple_qc_path = character(0)
		if (!is_purple_pp_range_null) {
			extracted_purple_qc_path = dir(dirname(as.character(row$purple_pp_range)), full.names = TRUE, pattern = ".qc$")
		}
		if (is_purple_qc_null && NROW(extracted_purple_qc_path) > 0) {
			purple_qc_path_for_fread = extracted_purple_qc_path[1]
		}
		
		is_purple_qc_path_valid = NROW(purple_qc_path_for_fread) == 1 && is.character(purple_qc_path_for_fread) && file.exists(purple_qc_path_for_fread)
		if (is_purple_qc_path_valid) {
			inferred_sex_field = fread(purple_qc_path_for_fread, header = FALSE)[V1 == "AmberGender"]$V2
			inferred_sex_field = tools::toTitleCase(tolower(inferred_sex_field))
		}
        
        futile.logger::flog.threshold("ERROR")
        tryCatchLog({

            # Create metadata object

            metadata <- create_metadata(
                pair = row$pair,
                tumor_type = row$tumor_type,
                tumor_details = row$tumor_details,
                disease = row$disease,
                primary_site = row$primary_site,
                inferred_sex = inferred_sex_field,
                purple_pp_bestFit = row$purple_pp_bestFit,
                jabba_gg = row[[jabba_column]],
                events = row$events,
                somatic_snvs = snvs_column,
                germline_snvs = row$germline_snvs,
                foreground_col_name = row$denoised_coverage_field,
                tumor_coverage = row$tumor_coverage,
                estimate_library_complexity = row$estimate_library_complexity,
                alignment_summary_metrics = row$alignment_summary_metrics,
                insert_size_metrics = row$insert_size_metrics,
                tumor_wgs_metrics = row$tumor_wgs_metrics,
                normal_wgs_metrics = row$normal_wgs_metrics,
                het_pileups = row$het_pileups,
                activities_sbs_signatures = row$activities_sbs_signatures,
                activities_indel_signatures = row$activities_indel_signatures,
                hrdetect = row$hrdetect,
                onenesstwoness = row$onenesstwoness,
                msisensorpro = row$msisensorpro,
                seqnames_genome_width_or_genome_length = genome_length,
                denoised_coverage_field = row$denoised_coverage_field,
                is_visible = row$metadata_is_visible,
                conpair_contamination = row$conpair_contamination,
                conpair_concordance = row$conpair_concordance,
				summary = row$string_summary,
                cohort_type = cohort_type,
                qc_flags_config = row$qc_flags[[1]]
            )

            if (is.null(metadata)) {
                print(sprintf("No metadata generated for %s", row$pair))
                return()
            }
            
            # Write to JSON
            jsonlite::write_json(
                metadata,
                out_file,
                auto_unbox = TRUE,
                pretty = TRUE,
                null = "null"
            )

			return(metadata)
            
        }, error = function(e) {
            print(sprintf("Error processing %s: %s", row$pair, e$message))
            NULL
        })
    }, mc.cores = cores, mc.preschedule = TRUE)

	metadata_tbls = rbindlist(list_metadata, fill = TRUE)
	cohort$inputs = Skilift::merge.repl(cohort$inputs, metadata_tbls, by = "pair", prefer_x = FALSE, prefer_y = TRUE)

    # invisible(NULL)

    do_lift_datafiles_json = identical(do_lift_datafiles_json, TRUE)

    if (do_lift_datafiles_json && TRUE) ## excessive checking to make sure nothing dumb happens
        Skilift::lift_datafiles_json(output_data_dir = output_data_dir, cores = cores)
    
	return(cohort)
}

#' @name lift_datafiles_json
#' @title lift_datafiles_json
#' @description
#' Create a combined JSON file for all data files in a directory
#'
#' @param data_dir Directory containing data files
#' @return None
#' @export
lift_datafiles_json <- function(output_data_dir, cores = 1) {
  if (!dir.exists(output_data_dir)) {
    stop("Data directory does not exist.")
  }
  
  # Recursively look for all files named "metadata.json"
  metadata_files <- list.files(
    path = output_data_dir,
    pattern = "metadata\\.json$",
    recursive = TRUE,
    full.names = TRUE
  )
  
  if (length(metadata_files) == 0) {
    stop("No metadata.json files found in the specified directory.")
  }
  
  # Read each JSON file and combine them into a list
  combined_data <- mclapply(metadata_files, function(file) {
    unbox(jsonlite::fromJSON(file))
  }, mc.cores = cores)

  # Write the combined JSON list to "datafiles.json" in the data directory
  output_file <- file.path(output_data_dir, "datafiles.json")
  jsonlite::write_json(combined_data, output_file, auto_unbox = TRUE, pretty = TRUE, null = "null")

  invisible(convert_json_to_arrow(output_file))
  
}


#' Convert a JSON file to an Arrow file.
#'
#' @param json_file_path Path to the input JSON file.
#' @param arrow_file_path Optional. Path to the output Arrow file.
#'   If NULL, the output path will be the same as the input JSON file,
#'   but with the .arrow extension.
#'
#' @return Invisibly returns the path to the created Arrow file.
#' @author Shihab Dider
#' @export
convert_json_to_arrow <- function(json_file_path, arrow_file_path = NULL) {
  # Validate input file path
  if (!file.exists(json_file_path)) {
    stop("JSON file not found: ", json_file_path)
  }

  # Determine output file path if not provided
  if (is.null(arrow_file_path)) {
    arrow_file_path <- sub("\\.json$", ".arrow", json_file_path, ignore.case = TRUE)
    if (arrow_file_path == json_file_path) {
      # In case the extension wasn't .json or sub failed
      arrow_file_path <- paste0(json_file_path, ".arrow")
    }
  }

  # Read JSON data
  tryCatch({
    data <- jsonlite::read_json(json_file_path, simplifyDataFrame = TRUE)
  }, error = function(e) {
    stop("Error reading JSON file: ", e$message)
  })
  
  # Ensure data is a data.frame
  if (!is.data.frame(data)) {
    tryCatch({
      data <- as.data.frame(data)
    }, error = function(e) {
      stop("Could not convert JSON content to a data.frame: ", e$message)
    })
  }

  # Pre-process list columns that might cause issues with Arrow type inference
  for (col_name in names(data)) {
    # Only process actual list-columns, not columns that are themselves data.frames
    if (is.list(data[[col_name]]) && !is.data.frame(data[[col_name]])) {
      is_simple_empty <- function(el) {
        is.null(el) ||
        (is.atomic(el) && length(el) == 0) ||
        (is.list(el) && !is.data.frame(el) && length(el) == 0)
      }
      
      all_elements_empty <- all(sapply(data[[col_name]], is_simple_empty))
      
      if (all_elements_empty) {
        cat(paste("Transforming column:", col_name, "to a vector of NA_character_ because all its elements were simple_empty.\n"))
        # Convert to a simple vector of NA_character_ of the correct length
        data[[col_name]] <- rep(NA_character_, length(data[[col_name]]))
      }
    }
  }

  # Write Arrow file
  tryCatch({
    arrow::write_feather(data, arrow_file_path, compression = "uncompressed")
    message("Successfully converted ", json_file_path, " to ", arrow_file_path)
  }, error = function(e) {
    message("Error writing Arrow file: ", e$message)
  })

  return(invisible(data))
}
